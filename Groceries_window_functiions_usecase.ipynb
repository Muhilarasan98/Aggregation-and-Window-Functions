{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49792242",
   "metadata": {},
   "source": [
    "Grouping and various aggregate functions\n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb12783",
   "metadata": {},
   "source": [
    " Wecan categorise the aggregate functions into 3 types as mentioned below.\n",
    " \n",
    " \n",
    " ● Simple Function\n",
    " ● Grouping Function\n",
    " ● Windowsfunction\n",
    " \n",
    " Also there are three types to write the functions in Apache Spark.\n",
    " ● Programmatic Style\n",
    " ● ColumnExpression Style\n",
    " ● Spark Sql Style\n",
    " \n",
    " Simple Function & Grouping Function:\n",
    " This function is used to get a single output out of the entire dataframe. Mostly the\n",
    " output will be the final metrics and no further transformation is required\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5df98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.shuffle.useOldFetchProtocol','true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03e05b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate functions on sales data\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferSchema\",True) \\\n",
    ".load(\"/public/trendytech/groceries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bd3a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+\n",
      "|order_id| location|    item|order_date|quantity|\n",
      "+--------+---------+--------+----------+--------+\n",
      "|      o1|  Seattle| Bananas|01/01/2017|       7|\n",
      "|      o2|     Kent|  Apples|02/01/2017|      20|\n",
      "|      o3| Bellevue| Flowers|02/01/2017|      10|\n",
      "|      o4|  Redmond|    Meat|03/01/2017|      40|\n",
      "|      o5|  Seattle|Potatoes|04/01/2017|       9|\n",
      "|      o6| Bellevue|   Bread|04/01/2017|       5|\n",
      "|      o7|  Redmond|   Bread|05/01/2017|       5|\n",
      "|      o8| Issaquah|   Onion|05/01/2017|       4|\n",
      "|      o9|  Redmond|  Cheese|05/01/2017|      15|\n",
      "|     o10| Issaquah|   Onion|06/01/2017|       4|\n",
      "|     o11|   Renton|   Bread|05/01/2017|       5|\n",
      "|     o12| Issaquah|   Onion|07/01/2017|       4|\n",
      "|     o13|Sammamish|   Bread|07/01/2017|       5|\n",
      "|     o14| Issaquah|  Tomato|07/01/2017|       6|\n",
      "|     o15| Issaquah|    Meat|08/01/2017|       3|\n",
      "|     o16| Issaquah|    Meat|09/01/2017|       5|\n",
      "|     o17| Issaquah|    Meat|10/01/2017|       6|\n",
      "|     o18| Bellevue|   Bread|11/01/2017|       7|\n",
      "|     o19| Bellevue|   Bread|12/01/2017|      54|\n",
      "|     o20| Bellevue|   Bread|13/01/2017|      34|\n",
      "+--------+---------+--------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2db85",
   "metadata": {},
   "source": [
    "use all of the three types of the Simple function such as Count, Count Distinct, Sum, Average and Grouping Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df0ab63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "012035f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+------------+\n",
      "|row_count|UniqueItems|total quantities|Avg quantity|\n",
      "+---------+-----------+----------------+------------+\n",
      "|       21|          9|             273|        13.0|\n",
      "+---------+-----------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Simple aggregation using progrmmatic style\n",
    "\n",
    "sap_df = df.select(count(\"*\").alias(\"row_count\"),countDistinct(\"item\").alias(\"UniqueItems\"), \\\n",
    "              sum(\"quantity\").alias(\"total quantities\"), avg(\"quantity\").alias(\"Avg quantity\"))\n",
    "sap_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3671bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------+\n",
      "| location|    item|Total Quantity|\n",
      "+---------+--------+--------------+\n",
      "|     Kent|  Apples|            20|\n",
      "|Sammamish|   Bread|             5|\n",
      "| Issaquah|    Meat|            14|\n",
      "|   Renton|   Bread|             5|\n",
      "|  Seattle| Bananas|             7|\n",
      "|  Seattle|Potatoes|             9|\n",
      "|  Redmond|  Cheese|            15|\n",
      "| Issaquah|   Onion|            12|\n",
      "|  Redmond|   Bread|             5|\n",
      "|  Redmond|    Meat|            40|\n",
      "| Bellevue| Flowers|            10|\n",
      "| Issaquah|  Tomato|             6|\n",
      "| Bellevue|   Bread|           125|\n",
      "+---------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## grouping function progrmmatic style\n",
    "\n",
    "sum_df = df.groupBy(\"location\",\"item\").agg(sum(\"quantity\").alias(\"Total Quantity\"))\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952934f3",
   "metadata": {},
   "source": [
    " By using this approach we can call the built in functions like Count, Count Distinct,\n",
    " Sum, Average and Agg and write the code in a programmatic way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59675627",
   "metadata": {},
   "source": [
    " Column Expression Style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07c6b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------+\n",
      "|row_number|unique_items|total_quantity|avg_quantity|\n",
      "+----------+------------+--------------+------------+\n",
      "|        21|           9|           273|        13.0|\n",
      "+----------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_df = df.selectExpr(\"count(*) as row_number\", \"count(Distinct(item)) as unique_items\", \\\n",
    "                      \"sum(quantity) as total_quantity\", \"avg(quantity) as avg_quantity\")\n",
    "col_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80d17527",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping function column expression style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2760da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------+\n",
      "| location|    item|total_quantity|\n",
      "+---------+--------+--------------+\n",
      "|     Kent|  Apples|            20|\n",
      "|Sammamish|   Bread|             5|\n",
      "| Issaquah|    Meat|            14|\n",
      "|   Renton|   Bread|             5|\n",
      "|  Seattle| Bananas|             7|\n",
      "|  Seattle|Potatoes|             9|\n",
      "|  Redmond|  Cheese|            15|\n",
      "| Issaquah|   Onion|            12|\n",
      "|  Redmond|   Bread|             5|\n",
      "|  Redmond|    Meat|            40|\n",
      "| Bellevue| Flowers|            10|\n",
      "| Issaquah|  Tomato|             6|\n",
      "| Bellevue|   Bread|           125|\n",
      "+---------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df1 = df.groupBy(\"location\",\"item\").agg(expr(\"sum(quantity) as total_quantity\"))\n",
    "sum_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786d1b1",
   "metadata": {},
   "source": [
    "Spark sql style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45360072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"groceries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "663df2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------+------------+\n",
      "|row_count|unique_items|total_quantity|avg_quantity|\n",
      "+---------+------------+--------------+------------+\n",
      "|       21|           9|           273|        13.0|\n",
      "+---------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) as row_count, count(distinct(item)) as unique_items, \n",
    "sum(quantity) as total_quantity, avg(quantity) as avg_quantity from groceries\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d36754a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------+\n",
      "| location|    item|total_quantity|\n",
      "+---------+--------+--------------+\n",
      "|     Kent|  Apples|            20|\n",
      "|Sammamish|   Bread|             5|\n",
      "| Issaquah|    Meat|            14|\n",
      "|   Renton|   Bread|             5|\n",
      "|  Seattle| Bananas|             7|\n",
      "|  Seattle|Potatoes|             9|\n",
      "|  Redmond|  Cheese|            15|\n",
      "| Issaquah|   Onion|            12|\n",
      "|  Redmond|   Bread|             5|\n",
      "|  Redmond|    Meat|            40|\n",
      "| Bellevue| Flowers|            10|\n",
      "| Issaquah|  Tomato|             6|\n",
      "| Bellevue|   Bread|           125|\n",
      "+---------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## grouping function in spark sql style\n",
    "\n",
    "spark.sql(\"\"\" select location, item, sum(quantity) as total_quantity\n",
    "from groceries group by location, item\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f05ec",
   "metadata": {},
   "source": [
    "Windows Function:\n",
    " This type of function is little advanced from the above function. Running Total, Rank,\n",
    " Dense Rank, Lead & Lag are some examples of the windows function.\n",
    " To use the window function we need to first set the window in which the aggregation\n",
    " should work.\n",
    " Window properties :\n",
    " 1. Partition by =>This will help to partition the data by a specified columns\n",
    " 2. Order By => We can use any Measure to order the partitioned column either\n",
    " Ascending or Descending.\n",
    " 3. Window Size => Specifying the size of the window is very important here, as\n",
    " this will play a crucial role to get out function to work as per the requirement.\n",
    " ● Current Row => This property will call the measure from the current\n",
    " row\n",
    " ● Unbounded Preceding => This property will call the measure from the\n",
    " first row\n",
    " ● Unbounded Following => This property will call the measure from the\n",
    " last row\n",
    " ● Likewise we can use-1 or 1 to change the direction of the row in which\n",
    " the function needs to select the row value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d676f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+-------------+\n",
      "|order_id| location|    item|order_date|quantity|running_total|\n",
      "+--------+---------+--------+----------+--------+-------------+\n",
      "|     o15| Issaquah|    Meat|08/01/2017|       3|            3|\n",
      "|     o16| Issaquah|    Meat|09/01/2017|       5|            8|\n",
      "|     o17| Issaquah|    Meat|10/01/2017|       6|           14|\n",
      "|      o8| Issaquah|   Onion|05/01/2017|       4|           18|\n",
      "|     o10| Issaquah|   Onion|06/01/2017|       4|           22|\n",
      "|     o12| Issaquah|   Onion|07/01/2017|       4|           26|\n",
      "|     o14| Issaquah|  Tomato|07/01/2017|       6|           32|\n",
      "|     o13|Sammamish|   Bread|07/01/2017|       5|            5|\n",
      "|      o7|  Redmond|   Bread|05/01/2017|       5|            5|\n",
      "|      o9|  Redmond|  Cheese|05/01/2017|      15|           20|\n",
      "|      o4|  Redmond|    Meat|03/01/2017|      40|           60|\n",
      "|      o1|  Seattle| Bananas|01/01/2017|       7|            7|\n",
      "|      o5|  Seattle|Potatoes|04/01/2017|       9|           16|\n",
      "|      o2|     Kent|  Apples|02/01/2017|      20|           20|\n",
      "|      o6| Bellevue|   Bread|04/01/2017|       5|            5|\n",
      "|     o18| Bellevue|   Bread|11/01/2017|       7|           12|\n",
      "|     o19| Bellevue|   Bread|12/01/2017|      54|           66|\n",
      "|     o20| Bellevue|   Bread|13/01/2017|      34|          100|\n",
      "|     o21| Bellevue|   Bread|14/01/2017|      25|          125|\n",
      "|      o3| Bellevue| Flowers|02/01/2017|      10|          135|\n",
      "+--------+---------+--------+----------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Running total\n",
    "window_spec = Window.partitionBy(\"location\").orderBy(\"item\") \\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "sum_df2 = df.withColumn(\"running_total\", sum(\"quantity\").over(window_spec))\n",
    "sum_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09a342",
   "metadata": {},
   "source": [
    "We are using the same grocery data to see the running total of the quantity partitioned\n",
    "by location, and ordered by Item. Window size is called from the current row to the\n",
    "previous row.\n",
    "Wecan also use the same Running total in Spark SQL style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e99eff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+--------+-------------+\n",
      "| location|    item|order_date|quantity|running_total|\n",
      "+---------+--------+----------+--------+-------------+\n",
      "| Issaquah|    Meat|08/01/2017|       3|            3|\n",
      "| Issaquah|    Meat|09/01/2017|       5|            8|\n",
      "| Issaquah|    Meat|10/01/2017|       6|           14|\n",
      "| Issaquah|   Onion|05/01/2017|       4|           18|\n",
      "| Issaquah|   Onion|06/01/2017|       4|           22|\n",
      "| Issaquah|   Onion|07/01/2017|       4|           26|\n",
      "| Issaquah|  Tomato|07/01/2017|       6|           32|\n",
      "|Sammamish|   Bread|07/01/2017|       5|            5|\n",
      "|  Redmond|   Bread|05/01/2017|       5|            5|\n",
      "|  Redmond|  Cheese|05/01/2017|      15|           20|\n",
      "|  Redmond|    Meat|03/01/2017|      40|           60|\n",
      "|  Seattle| Bananas|01/01/2017|       7|            7|\n",
      "|  Seattle|Potatoes|04/01/2017|       9|           16|\n",
      "|     Kent|  Apples|02/01/2017|      20|           20|\n",
      "| Bellevue|   Bread|04/01/2017|       5|            5|\n",
      "| Bellevue|   Bread|11/01/2017|       7|           12|\n",
      "| Bellevue|   Bread|12/01/2017|      54|           66|\n",
      "| Bellevue|   Bread|13/01/2017|      34|          100|\n",
      "| Bellevue|   Bread|14/01/2017|      25|          125|\n",
      "| Bellevue| Flowers|02/01/2017|      10|          135|\n",
      "+---------+--------+----------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## running total on spark sql\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        location, \n",
    "        item, \n",
    "        order_date, \n",
    "        quantity,\n",
    "        SUM(quantity) OVER(PARTITION BY location ORDER BY item, order_date) AS running_total\n",
    "    FROM groceries\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72bdd7",
   "metadata": {},
   "source": [
    "Rank, Dense Rank & RowNumber:\n",
    "\n",
    " Wecan use the same window properties to work the Rank, Dense Rank & Row\n",
    " number. But we need to call the specific function when creating the columns and use\n",
    " them accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff4e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d9d0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"location\").orderBy(\"Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccea71cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+----------+----+\n",
      "|order_id| location|    item|order_date|quantity|row_number|rank|\n",
      "+--------+---------+--------+----------+--------+----------+----+\n",
      "|     o15| Issaquah|    Meat|08/01/2017|       3|         1|   1|\n",
      "|      o8| Issaquah|   Onion|05/01/2017|       4|         2|   2|\n",
      "|     o10| Issaquah|   Onion|06/01/2017|       4|         3|   2|\n",
      "|     o12| Issaquah|   Onion|07/01/2017|       4|         4|   2|\n",
      "|     o16| Issaquah|    Meat|09/01/2017|       5|         5|   5|\n",
      "|     o14| Issaquah|  Tomato|07/01/2017|       6|         6|   6|\n",
      "|     o17| Issaquah|    Meat|10/01/2017|       6|         7|   6|\n",
      "|     o13|Sammamish|   Bread|07/01/2017|       5|         1|   1|\n",
      "|      o7|  Redmond|   Bread|05/01/2017|       5|         1|   1|\n",
      "|      o9|  Redmond|  Cheese|05/01/2017|      15|         2|   2|\n",
      "|      o4|  Redmond|    Meat|03/01/2017|      40|         3|   3|\n",
      "|      o1|  Seattle| Bananas|01/01/2017|       7|         1|   1|\n",
      "|      o5|  Seattle|Potatoes|04/01/2017|       9|         2|   2|\n",
      "|      o2|     Kent|  Apples|02/01/2017|      20|         1|   1|\n",
      "|      o6| Bellevue|   Bread|04/01/2017|       5|         1|   1|\n",
      "|     o18| Bellevue|   Bread|11/01/2017|       7|         2|   2|\n",
      "|      o3| Bellevue| Flowers|02/01/2017|      10|         3|   3|\n",
      "|     o21| Bellevue|   Bread|14/01/2017|      25|         4|   4|\n",
      "|     o20| Bellevue|   Bread|13/01/2017|      34|         5|   5|\n",
      "|     o19| Bellevue|   Bread|12/01/2017|      54|         6|   6|\n",
      "+--------+---------+--------+----------+--------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_ranks = df.select(\"order_id\",\"location\",\"item\",\"order_date\",\"quantity\") \\\n",
    "                .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "                .withColumn(\"rank\", rank().over(window_spec))\n",
    "df_with_ranks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b0275d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## same logic but in spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76c3a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+----------+----+----------+\n",
      "|order_id|location |item    |order_date|quantity|row_number|rank|dense_rank|\n",
      "+--------+---------+--------+----------+--------+----------+----+----------+\n",
      "|o15     |Issaquah |Meat    |08/01/2017|3       |1         |1   |1         |\n",
      "|o8      |Issaquah |Onion   |05/01/2017|4       |2         |2   |2         |\n",
      "|o10     |Issaquah |Onion   |06/01/2017|4       |3         |2   |2         |\n",
      "|o12     |Issaquah |Onion   |07/01/2017|4       |4         |2   |2         |\n",
      "|o16     |Issaquah |Meat    |09/01/2017|5       |5         |5   |3         |\n",
      "|o14     |Issaquah |Tomato  |07/01/2017|6       |6         |6   |4         |\n",
      "|o17     |Issaquah |Meat    |10/01/2017|6       |7         |6   |4         |\n",
      "|o13     |Sammamish|Bread   |07/01/2017|5       |1         |1   |1         |\n",
      "|o7      |Redmond  |Bread   |05/01/2017|5       |1         |1   |1         |\n",
      "|o9      |Redmond  |Cheese  |05/01/2017|15      |2         |2   |2         |\n",
      "|o4      |Redmond  |Meat    |03/01/2017|40      |3         |3   |3         |\n",
      "|o1      |Seattle  |Bananas |01/01/2017|7       |1         |1   |1         |\n",
      "|o5      |Seattle  |Potatoes|04/01/2017|9       |2         |2   |2         |\n",
      "|o2      |Kent     |Apples  |02/01/2017|20      |1         |1   |1         |\n",
      "|o6      |Bellevue |Bread   |04/01/2017|5       |1         |1   |1         |\n",
      "|o18     |Bellevue |Bread   |11/01/2017|7       |2         |2   |2         |\n",
      "|o3      |Bellevue |Flowers |02/01/2017|10      |3         |3   |3         |\n",
      "|o21     |Bellevue |Bread   |14/01/2017|25      |4         |4   |4         |\n",
      "|o20     |Bellevue |Bread   |13/01/2017|34      |5         |5   |5         |\n",
      "|o19     |Bellevue |Bread   |12/01/2017|54      |6         |6   |6         |\n",
      "+--------+---------+--------+----------+--------+----------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        location,\n",
    "        item,\n",
    "        order_date,\n",
    "        quantity,\n",
    "        ROW_NUMBER() OVER (PARTITION BY location ORDER BY quantity) AS row_number,\n",
    "        RANK()       OVER (PARTITION BY location ORDER BY quantity) AS rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY location ORDER BY quantity) AS dense_rank\n",
    "    FROM groceries\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d19f1b",
   "metadata": {},
   "source": [
    "Rank:\n",
    " Rank function is used when there is same values for two categories and we want to\n",
    " rank them with the same number, but skip the following Rank. This kind of method is\n",
    " used in Races.\n",
    " \n",
    " Dense Rank:\n",
    " Dense Rank also works similar to the Rank function, but this function will not skip the\n",
    " next rank and assign the same rank to repeating values.\n",
    " \n",
    " RowNumber:\n",
    " RowNumber is widely used inorder to find the Top N values of a specific partitioned\n",
    " value. This will however give unique value to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bacb581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+-------------+-------------+----------------+--------------+\n",
      "|order_id|location |item    |order_date|quantity|prev_quantity|next_quantity|change_from_prev|change_to_next|\n",
      "+--------+---------+--------+----------+--------+-------------+-------------+----------------+--------------+\n",
      "|o8      |Issaquah |Onion   |05/01/2017|4       |null         |4            |null            |0             |\n",
      "|o10     |Issaquah |Onion   |06/01/2017|4       |4            |4            |0               |0             |\n",
      "|o12     |Issaquah |Onion   |07/01/2017|4       |4            |6            |0               |2             |\n",
      "|o14     |Issaquah |Tomato  |07/01/2017|6       |4            |3            |2               |-3            |\n",
      "|o15     |Issaquah |Meat    |08/01/2017|3       |6            |5            |-3              |2             |\n",
      "|o16     |Issaquah |Meat    |09/01/2017|5       |3            |6            |2               |1             |\n",
      "|o17     |Issaquah |Meat    |10/01/2017|6       |5            |null         |1               |null          |\n",
      "|o13     |Sammamish|Bread   |07/01/2017|5       |null         |null         |null            |null          |\n",
      "|o4      |Redmond  |Meat    |03/01/2017|40      |null         |5            |null            |-35           |\n",
      "|o7      |Redmond  |Bread   |05/01/2017|5       |40           |15           |-35             |10            |\n",
      "|o9      |Redmond  |Cheese  |05/01/2017|15      |5            |null         |10              |null          |\n",
      "|o1      |Seattle  |Bananas |01/01/2017|7       |null         |9            |null            |2             |\n",
      "|o5      |Seattle  |Potatoes|04/01/2017|9       |7            |null         |2               |null          |\n",
      "|o2      |Kent     |Apples  |02/01/2017|20      |null         |null         |null            |null          |\n",
      "|o3      |Bellevue |Flowers |02/01/2017|10      |null         |5            |null            |-5            |\n",
      "|o6      |Bellevue |Bread   |04/01/2017|5       |10           |7            |-5              |2             |\n",
      "|o18     |Bellevue |Bread   |11/01/2017|7       |5            |54           |2               |47            |\n",
      "|o19     |Bellevue |Bread   |12/01/2017|54      |7            |34           |47              |-20           |\n",
      "|o20     |Bellevue |Bread   |13/01/2017|34      |54           |25           |-20             |-9            |\n",
      "|o21     |Bellevue |Bread   |14/01/2017|25      |34           |null         |-9              |null          |\n",
      "+--------+---------+--------+----------+--------+-------------+-------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"location\").orderBy(\"order_date\")\n",
    "\n",
    "df_with_lead_lag = df.withColumn(\"prev_quantity\", lag(\"quantity\").over(window_spec)) \\\n",
    "                     .withColumn(\"next_quantity\", lead(\"quantity\").over(window_spec)) \\\n",
    "                     .withColumn(\"change_from_prev\", col(\"quantity\") - lag(\"quantity\").over(window_spec)) \\\n",
    "                     .withColumn(\"change_to_next\", lead(\"quantity\", 1).over(window_spec) - col(\"quantity\"))\n",
    "\n",
    "df_with_lead_lag.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5502b",
   "metadata": {},
   "source": [
    "Lead &LagFunctions:\n",
    "These two functions are used to play with measures and find some insights.\n",
    "Lead => Lead helps to compare and get the next value in a new column of a\n",
    "partitioned data.\n",
    "\n",
    "Lag => likewise lead, lag also helps us to get the previous value of the partitioned\n",
    "data.\n",
    "\n",
    "We also need to use the Window properties with partition and order by in order to\n",
    "work with Lead & La"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ddaa3",
   "metadata": {},
   "source": [
    "Dealing With Null values in Apache Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbf928",
   "metadata": {},
   "source": [
    "LikeWise in any form of data we have the same problem of NULL and nan values in\n",
    " Apache spark Data frames. Identifying and dealing with them wisely would help us to\n",
    " maintain the data.\n",
    " Let's look at the sales data below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f02432a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "|      20|         G|    null|    NaN|\n",
      "+--------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_schema = \"store_id long, product String, quantity int, revenue double\"\n",
    "sales_df = spark.read.format(\"json\").schema(sales_schema).option(\"header\",\"True\")\\\n",
    ".load(\"/public/trendytech/datasets/sales_data.json\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3b7a4",
   "metadata": {},
   "source": [
    "The sales data has Null values in all of the columns. And it is present in both String\n",
    "and Integer data types.\n",
    " \n",
    "Type 1:\n",
    "We can change the null values in the integer data type by replacing it with any value.\n",
    "\n",
    "Here the replacement value majorly would be 0, and in some scenarios it can be a\n",
    "mean of a particular category. This will be dependent on the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95fe299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "|      20|         G|       0|    0.0|\n",
      "+--------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df = sales_df.fillna(0)\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45519f",
   "metadata": {},
   "source": [
    "Here we have filled the null values with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c26c5",
   "metadata": {},
   "source": [
    " Type 2:\n",
    " If we find any null value is not helping or providing any insights to us, then we can\n",
    " simply delete them by dropping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.filter(isnull(\"quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee7e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c7c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ea8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b8b8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba2a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
